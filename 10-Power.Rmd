# How big a sample do I need? Sampling, statistical power and type II errors {#power}

```{r, echo=F,warning=F,message=F}

library(tidyverse)
library(kableExtra)
library(knitr)
library(MASS)
library(dplyr)
library(magrittr)
library(ggplot2)
library(ggpubr)
library(pwr)
library(beeswarm) #nice plotting package
library(doBy)
library(here)

```

`r knitr::include_graphics("images/logo_alone_new.png")`

**Learning objectives**: By the end of this chapter, you will be able to:

-   Appreciate how studies with small samples may lead us to wrongly conclude an intervention is ineffective

-   Understand how effect size is measured and how it relates to statistical power

-   Explain what a type II error is, and how it may arise when sample size is too small

**A note of warning**: this chapter is probably the most complex in the book, and introduces several statistical concepts that can be challenging. Readers without a statistical background may find it helpful to try some of the exercises with interactive websites recommended in Check Your Understanding (below) if struggling with material in the main text.

## Sampling

Consider the following scenario: You have two people who tried a weight loss program, Carbocut, which restricted carbohydrates, and who lost an average of 4 lb over 2 weeks. Another two people just tried sticking to three meals a day (Program 3M). They lost an average of 1 lb over 2 weeks. Would that convince you that carbohydrate restriction is superior? What if we tell you that one person in the Carbocut group lost 6 lb and the other gained 2 lb; in the Program 3M group, one person lost 5 lb and one gained 3 lb. Few people would find this convincing evidence that Carbocut is superior. With only two people per group, the average is quite unstable and highly dependent on the specific people in the group. Things would look more promising if you had 1000 people in each group, and still saw a 4 lb loss vs a 2 lb loss. Individual weights would fluctuate, but the average is a much better indication of what is typical when we have a larger sample, because individual people with extreme scores will have less impact.

How could we know the real truth of just how much weight loss is associated with each diet? In theory, there is a way we could find out - we could recruit every single person in the world who meets some selection criterion for inclusion in the study, and randomly assign them to one or the other diet, and then measure weight loss. Of course, in practice, that is neither possible nor ethical. Rather, the aim of research is to test a group of people whose results can be generalized to the broader population. So we recruit a sample which is intended to represent that much larger population. As the example above demonstrates, we need to think about sample size, as this will affect how much confidence we can place in the estimates from the sample. We know that two people is too small a sample. 1000 seems much better but, outside medical trials, is often too expensive and difficult to achieve. So how do we decide the optimal sample? To answer that question, we first need to introduce the concept of **effect size**.

## Standardized effect size

Another way of thinking about our first example is that it illustrates that we can't interpret an average difference between two groups unless we know something about the variation within each group. In the example where we have only two people per group, the variation *within* each group (8 lb difference between highest and lowest) is far greater than the variation *between* groups (2 lb).

```{r demo-variation,echo=F,include=T,message=F,warning=F,fig.cap='Simulated data comparing two diets with different variances',fig.height=3,fig.width=8}

knitr::include_graphics("images_bw/demo_variation.png")

makefig <- 0 # we preserve code to make figure for just historical reasons. NB this did not use ggplot and so can't use ggsave. Uses base R to make figure.

if(makefig==1){
set.seed(9) #to ensure same values generated on each run - change to any other number for different results

#create a set of values to simulate for true group differences in SD units
truediff<-c(.1,.75,1.25) #here we have 3 values; c stands for cocatenate so we get 3 values
#in a vector; we can refer to them as truediff[1], truediff[2] and truediff[3]
truelabel<-c('A','B','C') #to label the plots for each run

ndiffs<-length(truediff)

par(mfrow=c(1,3))

myn<-20 #N per group
myES <- NA
myavgSD <- NA
for (i in 1:ndiffs){
  thisdiff<-truediff[i]
mynum <- rnorm(myn*2)
mynum[1:myn]<-mynum[1:myn]-thisdiff  #add truediff value to group 1 scores

#scale so that variances differ rather than means
mynum<-(-2)*mynum/(i)-2 #just divide by 1, 2 or 3, after switching polarity and multiplying by 2 (to make it more compatible with the example)

mygp<-c(rep(2,myn),rep(1,myn))#generate group IDs for equal sized groups 1 and 2
mydat<-data.frame(cbind(mygp,mynum)) #stick it all together in a dataframe
#mydat #uncomment this to look at the dataframe if you like

#Show the plot with the stats
myylab<-''
if (i==1){myylab<-'Weight change'}
mydat$mygp<-as.factor(mydat$mygp)
levels(mydat$mygp)<-c('Carbocut','3M')

beeswarm(mynum~mygp , data = mydat,xlab=truelabel[i],ylab=myylab,
         col='red',pch=14,ylim=c(-5,2),cex.axis=1.5,cex.lab=1.5)
segments(x0 = 0.7, x1 = 1.3,
         y0 = mean(mydat$mynum[21:40]), 
         lty = 1, lwd = 2,col='black')
segments(x0 = 1.7, x1 = 2.3,
         y0 = mean(mydat$mynum[1:20]), 
         lty = 1, lwd = 2,col='black')
myES[i]<-(mean(mydat$mynum[21:40])-mean(mydat$mynum[1:20]))/mean(sd(mydat$mynum[1:20]),sd(mydat$mynum[21:40]))
myavgSD[i]<-mean(sd(mydat$mynum[1:20]),sd(mydat$mynum[21:40]))

}
}

```

To give an understanding of the standardized effect size, let us first focus on the case where we have three studies each with the same sample size, 20, but where the amount of overlap between the samples is different from one study to the next. This is shown in Figure \@ref(fig:demo-variation), where the mean difference in lbs of weight loss between the two diets (represented by the difference in the black horizontal lines) is similar in each case, but the variation within each group (the spread of points around the line) is greatest in scenario A, and least in scenario C. *Relative to the variation within each group*, the difference in means is smallest in A, intermediate in B and largest in C. Another way of saying this is that the *standardized effect size* is different in the three scenarios.

There are various ways of measuring standardized effect size: a simple measure is **Cohen's *d***, which is the difference between groups measured in **standard deviation** units. The standard deviation is a measure of spread of data around the mean: it is largest for scenario A (SD for each group is around 2), intermediate for scenario B (SD is around 1) and smallest for scenario C (SD around .5). If you are unfamiliar with the concept of standard deviation, you can find out more [here](https://www.mathsisfun.com/data/standard-deviation.html).

Cohen's d is computed by dividing the mean difference between groups by the pooled standard deviation. For conditions A, B and C, Cohen's *d* is .4, .8 and 1.5, respectively. Note that in all three scenarios, the distributions of weight loss in the two groups overlap. In every case, there are people in the 3M group with weight loss greater than the mean of the Carbocut group, as well as people in the Carbocut group with less weight loss than the mean of the 3M group. Figure \@ref(fig:effsizefig) shows data as density plots, where the height of the distribution represents the frequency of individuals with that score. This shows how overlap is related to effect size: the larger the effect size, the less the overlap. It can be sobering to note that for most effective educational and therapeutic interventions, effect sizes are typically no more than .3 or .4. Thus there is a lot of overlap, and in effect we have to pick out a meaningful signal - an intervention effect - from a very noisy background.

```{r effsizefig, echo=F,out.width="100%", fig.cap="Overlap in distributions of scores of two groups for different effect sizes (Cohen's *d*)"}
#made with script makeESdemodensitych10.R
knitr::include_graphics('images/densityplot.png')
```

## Sample size affects accuracy of estimates

Once we have an idea of the likely effect size in our study, we can estimate how big a sample we need. If we have a small effect size, then we need many observations to get a reliable estimate of the true effect of intervention. Figure \@ref(fig:varES) resembles Figure \@ref(fig:effsizefig), but there is an important difference: in Figure \@ref(fig:varES), each blob represents the *observed mean* from a sample of a given size, taken from a population where the true effect size, shown as a dotted horizontal line, is 0.3.

```{r varES, echo=F,out.width="70%",fig.cap="Simulated mean scores from samples of varying size, drawn from populations with either a null effect (grey) or a true effect size, Cohen's *d*, of .3 (black). Power (discussed below) is the probability of obtaining p < .05 on a one-tailed t-test comparing group means for each sample size (Based on Bishop et al, 2022)"}
knitr::include_graphics('images_bw/beeswarms_ch10.png')

```

Notice how the observed means jump around from sample to sample when sample size is small, but become more stable once we have 80 or more participants per group. When there are 320 participants per group, the means are estimated pretty accurately and the dots bunch around the mean line for the group, but with 10 per group, it is very unreliable, and in some of our studies the mean of the black group is lower than the mean of the grey group, which is opposite to what is true in the population.

## Understanding p-values

When we do an intervention study, we want to find out whether a given intervention works. The examples above show that it is not enough just to show that the mean outcome for an intervention group is better than for a control group. We need to take the variation around the means into account.

Most studies use an approach known as Null Hypothesis Significance Testing, which gives us a rather roundabout answer to the question. Typically, findings are evaluated in terms of p-values, which tell us *what is the probability (p) that our result, or a more extreme one, could have arisen if there is no real effect of the intervention* - or in statistical jargon, if the **null hypothesis** is true. Suppose in a hypothetical world we have a totally ineffective drug, and we do 100 studies in large samples to see if it works. On average, in five of those studies (i.e. 5 per cent of them), the p-value would be below .05. And in one study (1 per cent), it would be below .01. The way that p-values are calculated assumes certain things hold true about the nature of the data ("model assumptions"): we will say more about this later on, and show how it is important to test assumptions when applying statistical tests, to avoid getting misleading results.

P-values are very often misunderstood, and there are plenty of instances of wrong definitions being given even in textbooks. The p-value is *the probability of the observed data or more extreme data, if the null hypothesis is true*. It does *not* tell us the probability of the null hypothesis being true. And it tells us nothing about the alternative hypothesis, i.e., that the intervention works.

An analogy might help here. Suppose you are a jet-setting traveller and you wake up one morning confused about where you are. You wonder if you are in Rio de Janiero - think of that as the null hypothesis. You look out of the window and it is snowing. You decide that it is very unlikely that you are in Rio. You reject the null hypothesis. But it's not clear where you are. Of course, if you knew for sure that you were either in Reykjavík or Rio, then you could be pretty sure you were in Reykjavík. But suppose it was *not* snowing. This would not leave you much the wiser.

A mistake often made when interpreting p-values is that people think it tells us something about the probability of a hypothesis being true. That is not the case. There are alternative Bayesian methods that can be used to judge the relatively likelihood of one hypothesis versus another, given some data, but they do not involve p-values.

A low p-value allows us to reject the null hypothesis with a certain degree of confidence, but this does no more than indicate "something is probably going on here - it's not just random" - or, in the analogy above, "I'm probably not in Rio".

#### Criticisms of the use of p-values {.unnumbered}

::: {#custom}
There are many criticisms of the use of p-values in science, and a good case can be made for using alternative approaches, notably methods based on Bayes theorem. Our focus here is on Null Hypothesis Significance Testing in part because such a high proportion of studies in the literature use this approach, and it is important to understand p-values in order to evaluate the literature. It has also been argued that p-values are useful provided people understand what they really mean [@lakens2021].

One reason why many people object to the use of p-values is that they are typically used to make a binary decision: we either accept or reject the null hypothesis, depending on whether the p-value is less than a certain value. In practice, evidence is graded, and it can be more meaningful to express results in terms of the amount of confidence in alternative interpretations, rather than as a single accept/reject cutoff [@quintana2018].
:::

In practice, p-values are typically used to divide results into "statistically significant" or "non-significant", depending on whether the p-value is low enough to reject the null hypothesis. We do not defend this practice, which can lead to an all-consuming focus on whether p-values are above or below a cutoff, rather than considering effect sizes and strength of evidence. However, it is important to appreciate how the cutoff approach leads to experimental results falling into 4 possible categories, as shown in Table \@ref(tab:confusionMat).

```{r confusionMat, echo=FALSE, message=FALSE, results='asis', warnings=FALSE}
options(kableExtra.html.bsTable = T)
mymat<-matrix(NA,nrow=2,ncol=3)

mymat[,1]<-c("Reject Null hypothesis","Do Not Reject Null Hypothesis")

mymat[,2]<-c("True Positive","False Negative \n (Type II error)")
mymat[,3]<-c("False Positive \n (Type I error)","True Negative")

mymat<-as.data.frame(mymat)
names(mymat)<-c("","Intervention effective","Intervention ineffective")

knitr::kable(mymat,escape = F, align = "c", booktabs = T, caption = 'Outcomes of hypothesis test') %>%
  kable_styling(c("striped", "bordered"), latex_options = "striped", full_width = F) %>%
  column_spec(1:1, bold = T) %>%
  column_spec(1:3,width = "9em") %>%
  add_header_above(c(" ","Ground truth" = 2))
```

The ground truth is the result that we would obtain if we were able to administer the intervention to the whole population - this is of course impossible, but we assume that there is some general truth that we are aiming to discover by running our study on a sample from the population. We can see that if the intervention really is effective, and the evidence leads us to reject the null hypothesis, we have a True Positive, and if the intervention is ineffective and we accept the null hypothesis, we have a True Negative. Our goal is to design our study so as to maximize the chances that our conclusion will be correct, but there two types of outcome that we can never avoid, but which we try to minimize, known as Type I and Type II errors. We will cover Type II errors in this chapter, as these depend crucially on sample size and effect size. Type I errors are covered in Chapter \@ref(phacking).

### Type II error

A Type II error is the same as a **false negative**. It is the error that occurs when the null hypothesis is not rejected but a true effect is actually present. In other words, the data lead us to conclude an intervention doesn't work when it really does have an effect.

Suppose a researcher wants to test a well-established and frequently-replicated result: children whose parents read to them more frequently obtain better results when they are tested on vocabulary. 20 families are split into two groups; in the first group, parents are encouraged to read with their child each night for 3 months, whereas in the other group no such encouragement is given. The study is run, but when children's vocabulary results are compared, the statistical test results in a p-value of .23, much greater than the $\alpha$ level of .05.

The researcher is confused as she knows there is research evidence to indicate that an effect should be present. There are, of course, a number of reasons why the experiment might have turned up a null result, but anyone familiar with statistics will think of the most likely explanation: unfortunately, she has failed to take into account the fact that the effect is fairly small, and to show it convincingly she would need a much larger sample size then 10 families per group.

## Statistical power and $\beta$

Statistical power is the probability that a study will show a significant difference on a statistical test when there is a true effect. Statisticians use the term $\beta$ to refer to the proportion of nonsignificant results that are false negatives (type II error); power is 1-$\beta$, expressed either as a proportion or a percentage.

In practice, it can get confusing to think about Greek symbols (especially since $\beta$ has a completely different statistical meaning in the context of regression!) but the main point to grasp is that if I say my study has 40% power, that means that, if there were a true effect of intervention, and I were to run the study 10 times, on only four occasions would I obtain a statistically significant difference.

Power depends several factors including:

-   Sample size
-   True effect size in the population
-   Criterion for statistical significance, also known as the Type 1 error rate ($\alpha$)

As can be seen from Table \@ref(tab:powertable), in general, the larger the sample size, the higher the power, and the greater the effect size, the higher the power. This is a sobering table for fields where it is not uncommon to have sample sizes of 20 or less per group, especially as we know that few effective interventions have effect sizes greater than .4.

```{r powertable,echo=FALSE, message=FALSE,warnings=FALSE}

options(kableExtra.html.bsTable = T)
nlist<-c(10,20,30,40,50,80,100,150,200)
elist<-c(.1,.2,.3,.4,.5,.6,.8,1)
powerdf<-data.frame(matrix(NA,nrow=length(nlist),ncol=(1+length(elist))))
colnames(powerdf)<-c('N per group',elist)
powerdf[,1]<-nlist
thisrow<-0

for (myn in nlist){
  thisrow<-thisrow+1
  thiscol<-1
  for (mye in elist){
    thiscol<-thiscol+1
    powerdf[thisrow,thiscol]<-round(pwr.t.test(n=myn,d=mye,sig.level=.05,type='two.sample')$power,3)
    
  }
}
knitr::kable(powerdf,escape = F, align = "c", booktabs = T, caption = 'Power for 2-tailed independent t-test, alpha = .05') %>%
  kable_styling(c("striped", "bordered"), latex_options = "striped", full_width = F)  %>%
  add_header_above(c(" ","Effect size (d)" = 8))
```

We can also show how power is affected by changing the $\alpha$ level - this affects how large a difference we need to see between groups before we reject the null hypothesis. When $\alpha$ is more extreme, we will make fewer false positive errors (see Chapter \@ref(phacking)), but we will make more false negatives.

Figure \@ref(fig:densplot) illustrates this in a simple example using a z-test, which simply assesses how likely it is that a sample came from a population with a given mean. This is not a realistic example, but it is used to give more insight into power. Suppose we have some background information with sets of reading scores from children who did and did not have the intervention. We now have a single child's score on a reading test, and we want to know whether they had the reading intervention. Figure \@ref(fig:densplot) below shows the distribution of scores for children who had no intervention in the top: this is a density plot, showing how common different scores are (proportion of the population is on the y-axis), for each specific score on the x-axis. The shape follows a normal distribution: most scores are in the middle, with higher or lower scores being less common, and the mean is zero and standard deviation is one. The null hypothesis is that the child's score comes from this distribution.

```{r densplot,echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Z test: statistical power, N=1"}
#https://stats.stackexchange.com/questions/14140/how-to-best-display-graphically-type-ii-beta-error-power-and-sample-size

#TeachingDemos package in R for inspiration.
knitr::include_graphics('images_bw/densplot-1.png')


```

The lower figure shows the distribution for children who had the reading intervention. The intervention had a large effect (Cohen's *d* of one), and so the whole distribution is shifted over to the right. We're going to use a one-sided z-test, because we know that the child's score will either come from the null distribution, or from one with a higher mean. We decide to use the conventional level of $\alpha$ of .05. The vertical dotted line is therefore placed at a point where 5% of the upper distribution (the red area) is to the right of the line, and 95% (the white area) to the left. This point can be worked out from knowledge of the normal distribution and corresponds to a score on the x-axis of 1.65. Our rule is that if the child's score is greater than 1.65, we reject the null hypothesis. If it is less than 1.65, we can't reject the null hypothesis. This does not mean that the child definitely came from the non-intervention group - just that the evidence is not sufficient to rule that out. Regardless of the effect size or the sample size, if $\alpha$ level is set to .05, we wrongly reject the null hypothesis on only 1 in 20 experiments.

But now look down to the lower distribution. False negatives are represented by the dark grey area to the left of the dotted line: cases where we fail to reject the null hypothesis, but the score really came from the group with intervention. The power of the study corresponds to the pale grey region, where we have a score greater than 1.65 and the data really come from the intervention condition. But notice that power is extremely low: the dark grey area is much bigger than the pale grey area. We are much more likely to wrongly retain the null hypothesis than to correctly reject it - even though we have a pretty large effect size. Note that whereas the false positive rate is kept constant by selection of $\alpha$ of .05, the false negative result is not.

If we wanted to make the false negative (type II error) rate much lower, we could adopt a less stringent $\alpha$ level, e.g. we could move the vertical dotted line to the point where the x-axis was zero, so the play grey area becomes much bigger than the dark grey area. But if we do that, we then would increase the type I (false positive) error rate to 50%!

```{r densplot2,echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Z test: statistical power, N=10"}
#created with makealphabeta2.R

#TeachingDemos package in R for inspiration.
knitr::include_graphics('images_bw/densplot-2.png')
```

Our next figure \@ref(fig:densplot2) presents the same one-sided z test but here the sample size has increased to 10. A key point is that the density plot here does *not* show the distribution of scores from individual children in any one sample; it is the distribution of *means* that we would see if we repeatedly took samples of a given size. So if we had a population of 10,000 children, and just kept taking samples of 10 children, each of those samples would have a mean, and it is these that are plotted here. We should notice that two things have appeared to change. First, we see a greater distinction between the two distributions. Second, we see that the critical $z$ value (vertical dashed line) has changed location. The distributions have not changed their location (the peak of each bell shaped curve is the same), but the spread of each distribution has shrunk as a result of the increased sample size, because the precision of the estimate of the mean improves with a larger sample.The shaded areas on the density plots directly relate to the concepts outlined above: power, type I, and type II errors. When the sample size increases, the standard error (SE) reduces. We notice that the type I error rate (area in red) is proportionally the same at 5%, but we see a change in the two remaining quantities, power and type II error rate. This is because these quantities are linked. The area under the density curve must always remain at 1, so proportionally, we can calculate the power as 1-$\beta$. We can visually see this in both figures by looking at the specified areas for the alternative distribution.

If you are finding this all quite confusing, don't worry. This is complicated and even those who have statistical training can find it challenging [@bishop2022a]. The most important points to take away from these examples are that:\
- Statistical power depends on the sample size and the effect size, as well as the level of $\alpha$\
- With small samples, studies often have low power, meaning that even if there is a real effect, there may be little chance of detecting it.\
- A p-value greater than .05 does not mean the null hypothesis is true.

It is therefore important to think about power when designing a study, or you may end up concluding an intervention is ineffective, when in fact it has a small effect that your study is underpowered to detect.

#### Standard error of the mean {.unnumbered}

<div>

It can be challenging to get an intuitive understanding of power, because the computations needed to calculate it are not straightforward. A key statistic is the standard error of the mean, also known as the SEM, usually shortened to standard error (SE). This can be thought of as an index of the variability of an estimate of the mean from a sample. If you imagine taking numerous samples from a population, and estimating the mean from each one, you will end up with a distribution of means, similar to those shown in Figure \@ref(fig:varES). As shown in that Figure, these estimates of the mean are much more variable for small than for large samples. The SE is the standard deviation of the estimates of the mean, and it is crucially dependent on the sample size. This follows from the formula for the SE, which is computed as the SD divided by the square root of N.

The test statistic, *z* in this instance, which is used to derive a p-value, uses the SE as a denominator, and so will also be influenced by sample size. The *z* score is defined as:

z = (M - $\mu$)/SE

The bigger the N, the smaller the SE, the more precise the measurement, and the higher the power of the statistical test. Note that the value entered into these equations is the *square root* of N. It follows that improvement in precision from adding extra participants to a study is greatest at small sample sizes.

```{r demoSE, echo=F, include=T,out.width="50%", message=FALSE, warning=FALSE, fig.cap="Plot showing how SE reduces with sample size (N) in a nonlinear fashion"}
#figure created simply in base R
#myseq <- seq(10,200,10)
#plot(myseq,1/sqrt(myseq),type='b',ylab='SE',xlab='N')
knitr::include_graphics('images_bw/demoSE.png')

```

</div>

<div>

As shown in Figure \@ref(fig:demoSE), the SE is approximately halved in increasing the sample size from 10 to 40, whereas changes are much smaller in going from 110 to 140 participants.Typically, clinical trials in medicine are designed to achieve 80% statistical power and, depending on the statistical analysis strategy, will employ some method to control type I error rate (traditionally $\alpha=0.05$). With $\alpha$ fixed, power depends on effect size and sample size.

</div>

So the first question is how do we select an effect size? This is quite a thorny issue. In the past, it was common just to base anticipated effect sizes on those obtained in previous studies, but these can be overestimates because of publication bias (see Chapter \@ref(pubbias)). A more logical approach is to consider what is the smallest effect size that would be of interest: for instance, if you have a vocabulary intervention on which children start with a mean score of 20 words (SD of 10) would you be interested in an average improvement on an outcome test of half a word, or would you think the intervention would only be worthwhile if children improved on average by 4 or more words? @lakens2021 has a useful primer on how to justify a sample size.

Once a suitable effect size is established, then it is possible to compute power for different effect sizes, to work out how big a sample would be needed to attain the desired statistical power, typically set to 80% or more.

## Ways to improve statistical power

Researchers are often shocked when they first do a power analysis, to find that sample sizes that are conventionally used in their field are not adequately powered. Even more depressing, a power analysis may tell you that you would need an unfeasibly large sample in order to show an effect of interest. Researchers who start out planning a study with 20 individuals per group may be discouraged to find that they need 80 per group to do a meaningful study. This is a common predicament, but there are some ways forward:

-   If the necessary sample size is too large for you to achieve, it may be worth considering forming a consortium by joining forces with other researchers. Kate Button has advocated for the adoption of **team science** in psychology, recognising that many questions require larger samples than are typically available in any one centre [@button2020]. A particularly innovative step has been to encourage consortia for undergraduate research projects, which, she argues, not only allows for meaningful research to be done, but also provides much better training in research methods than the conventional set-up, where each student is supposed to do an original piece of research with limited resources [@button2018].\
-   People tend to think that the only way to increase power is by getting a larger sample size, but there are other options. You may be able to improve the effect size of your outcomes by careful consideration of reliability and sensitivity of the outcome measure. Remember, effect size is the difference in means divided by the standard deviation: if you can reduce the standard deviation by minimising random noise in your outcome measure, you will increase the effect size.\
-   In Chapter \@ref(phacking), we will consider how use of multiple outcome measures can be used to improve statistical power, provided measures are taken to avoid increasing false positives.
-   It is worth consulting with a statistician about the optimal research design. A RCT with comparison of intervention and control groups is not the only possible approach. Some designs are more efficient than others for showing effects of interest: see especially Chapter \@ref(crossover) and Chapter \@ref(Single).

## Check your understanding

The teaching of statistics is being transformed by the internet. There are some excellent interactive websites to help you gain a more intuitive sense of some of the statistical concepts in this chapter. For starters, we'd recommend [this website](https://www.zoology.ubc.ca/~whitlock/Kingfisher/SamplingNormal.htm) which focuses on sampling from a normally distributed population. You can vary the mean and standard deviation, as well as the sample size. The examples are from zoology, but the ideas apply much more broadly. This is just one of [several apps in a set](https://whitlockschluter.zoology.ubc.ca/stats-visualizations) that is designed to help understand statistics.

There are several online resources for calculating power for simple experimental designs. They vary in the level of statistical background that they require, but you should be able to use them to work out the required sample size for different scenarios. Have a look at these sites:

\
[Power Calculator](https://egap.shinyapps.io/Power_Calculator/) Ignore the options of Clustered Design and Binary Dependent Variable for now. Also, note that when the Power Calculator asks you for the 'Treatment Effect Size' this is the *unstandardized* effect size, which is just the difference between means for Intervention and Control groups. If you specify that the standard deviation is 1, then you can get results for the standardized effect size, Cohen's d. Note that Power Calculator gives you the *total* number of participants. Be careful, because some power calculators give the number needed in each group.

\
[Cohen's d](https://shiny.ieis.tue.nl/d_p_power/)

See whether for each site you can understand the graphics, and whether you can work out the answer for the following problems, each of which is just comparing an intervention group and a control group in a two-sided t-test:\
- You have an expected effect size of 0.3 and you want to work out the sample size to get power of .8 with $\alpha$ set to .05.\
- You can recruit 50 participants per group; you expect an effect size of 0.3. What is the power to detect this effect, with $\alpha$ set to .05?\
- What effect size would you be able to detect with 80% power and a sample size of 30 per group, with $\alpha$ set to .05?

More generally, it's well worth Googling for information if there are statistical terms or concepts you are struggling with. Sometimes the information you find is just confusing and too technical, but, as illustrated above, there are also excellent teaching resources out there.
