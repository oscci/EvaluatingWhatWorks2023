# Analysis of a two-group RCT {#analysis}

```{r,echo=F,warning=F,message=F}
require(psych) #for describeBy
library(rstatix) #for ancova
library(yarrr) # for pirate plots (see below)
library(nlme)
library(lme4)
library(ggpubr)
library(kableExtra)
library(beeswarm)
require(MASS) #simulating multivariate data
```

```{r addlogo,echo=F,warning=F,message=F}
mylogo <- 0
if(mylogo==1){
knitr::include_graphics("images/logo_alone_new.png")
}
```

## Learning objectives  
By the end of this chapter, you will be able to:

-   Interpret output from a t-test
-   Understand why Analysis of Covariance is often recommended to analyze outcome data 

## Planning the analysis

Statisticians often complain that researchers will come along with a set of data and ask for advice as to how to analyse it. Sir Ronald Fisher (one of the most famous statisticians of all time) commented:

> "To consult the statistician after an experiment is finished is often merely to ask him to conduct a post mortem examination. He can perhaps say what the experiment died of."

-Sir Ronald Fisher, Presidential Address to the First Indian Statistical Congress, 1938.\index{statistical consulting}

His point was that very often the statistician would have advised doing something different in the first place, had they been consulted at the outset. Once the data are collected, it may be too late to rescue the study from a fatal flaw.

Many of those who train as allied health professionals get rather limited statistical training. We suspect it is not common for them to have ready access to expert advice from a statistician. We have, therefore, a dilemma: many of those who have to administer interventions have not been given the statistical training that is needed to evaluate their effectiveness.

We do not propose to try to turn readers of this book into expert statisticians, but we hope to instill a basic understanding of some key principles that will make it easier to read and interpret the research literature, and to have fruitful discussions with a statistician if you are planning a study.

## Understanding p-values

\index{p-values}When we do an intervention study, we want to find out whether a given intervention works. Most studies use an approach known as **Null Hypothesis Significance Testing**,\index{Null Hypothesis Significance Testing} which gives us a rather roundabout answer to the question. Typically, findings are evaluated in terms of p-values, which tell us *what is the probability (p) that our result, or a more extreme one, could have arisen if there is no real effect of the intervention* - or in statistical jargon, if the **null hypothesis**\index{null hypothesis} is true. The reason why we will sometimes find an apparent benefit of intervention, even when there is none, is down to random variation, as discussed in Chapter \ref{intro}. Suppose in a hypothetical world we have a totally ineffective drug, and we do 100 studies in large samples to see if it works. On average, in five of those studies (i.e. 5 per cent of them), the p-value would be below .05. And in one study (1 per cent), it would be below .01. The way that p-values are calculated assumes certain things hold true about the nature of the data ("model assumptions"): we will say more about this later on.

P-values are very often misunderstood, and there are plenty of instances of wrong definitions being given even in textbooks. The p-value is *the probability of the observed data or more extreme data, if the null hypothesis is true*. It does *not* tell us the probability of the null hypothesis being true. And it tells us nothing about the plausibility of the alternative hypothesis, i.e., that the intervention works.

An analogy might help here. Suppose you are a jet-setting traveller and you wake up one morning confused about where you are. You wonder if you are in Rio de Janiero - think of that as the null hypothesis. You look out of the window and it is snowing. You decide that it is very unlikely that you are in Rio. You reject the null hypothesis. But it's not clear where you are. Of course, if you knew for sure that you were either in Reykjavík or Rio, then you could be pretty sure you were in Reykjavík. But suppose it was *not* snowing. This would not leave you much the wiser.

A mistake often made when interpreting p-values is that people think it tells us something about the probability of a hypothesis being true. That is not the case. There are alternative Bayesian methods that can be used to judge the relatively likelihood of one hypothesis versus another, given some data, but they do not involve p-values.

A low p-value allows us to reject the null hypothesis with a certain degree of confidence, but this does no more than indicate "something is probably going on here - it's not just random" - or, in the analogy above, "I'm probably not in Rio".

<!---#### Criticisms of the use of p-values {.unnumbered}

<div>-->

\begin{tcolorbox}[colback=Black!5!lightgray,colframe=black!75!black,coltitle=white,title=Criticisms of the use of p-values]\label{box:pvalues}
There are many criticisms of the use of p-values in science, and a good case can be made for using alternative approaches, notably methods based on Bayes theorem. Our focus here is on Null Hypothesis Significance Testing in part because such a high proportion of studies in the literature use this approach, and it is important to understand p-values in order to evaluate the literature. It has also been argued that p-values are useful provided people understand what they really mean \protect\hyperlink{ref-lakens2021}{Lakens} (\protect\hyperlink{ref-lakens2021}{2021}).

One reason why many people object to the use of p-values is that they are typically used to make a binary decision: we either accept or reject the null hypothesis, depending on whether the p-value is less than a certain value. In practice, evidence is graded, and it can be more meaningful to express results in terms of the amount of confidence in alternative interpretations, rather than as a single accept/reject cutoff (\protect\hyperlink{ref-quintana2018}{Quintana \& Williams, 2018}).
\end{tcolorbox}
<!---</div>-->

In practice, p-values are typically used to divide results into "statistically significant" or "non-significant", depending on whether the p-value is low enough to reject the null hypothesis.\index{significance!statistical} We do not defend this practice, which can lead to an all-consuming focus on whether p-values are above or below a cutoff, rather than considering effect sizes and strength of evidence. However, it is important to appreciate how the cutoff approach leads to experimental results falling into 4 possible categories, as shown in Table \@ref(tab:confusionMat).

```{r confusionMat, echo=FALSE, message=FALSE, results='asis', warnings=FALSE}
options(kableExtra.html.bsTable = T)
mymat<-matrix(NA,nrow=2,ncol=3)

mymat[,1]<-c("Reject Null hypothesis","Do Not Reject Null Hypothesis")

mymat[,2]<-c("True Positive","False Negative \n (Type II error)")
mymat[,3]<-c("False Positive \n (Type I error)","True Negative")

mymat<-as.data.frame(mymat)
names(mymat)<-c("","Intervention effective","Intervention ineffective")

knitr::kable(mymat,escape = F, align = "c", booktabs = T, caption = 'Possible outcomes of hypothesis test') %>%
  kable_styling(c("striped", "bordered"), latex_options = "striped", full_width = F) %>%
  column_spec(1:1, bold = T) %>%
  column_spec(1:3,width = "9em") %>%
  add_header_above(c(" ","Ground truth" = 2))
```

The ground truth is the result that we would obtain if we were able to administer the intervention to the whole population - this is of course impossible, but we assume that there is some general truth that we are aiming to discover by running our study on a sample from the population. We can see that if the intervention really is effective, and the evidence leads us to reject the null hypothesis, we have a True Positive, and if the intervention is ineffective and we accept the null hypothesis, we have a True Negative. Our goal is to design our study so as to maximize the chances that our conclusion will be correct, but there two types of outcome that we can never avoid, but which we try to minimize, known as Type I and Type II errors.\index{Type I error}\index{false positives}\index{false negatives}\index{Type II error} We will cover Type II errors in Chapter \@ref(power) and Type I in Chapter \@ref(phacking).

## What kind of analysis is appropriate?

The answer to the question "How should I analyse my data?" depends crucially on what hypothesis is being tested. In the case of an intervention trial, the hypothesis will usually be "did intervention X make a difference to outcome Y in people with condition Z?" There is, in this case, a clear null hypothesis -- that the intervention was ineffective, and the outcome of the intervention group would have been just the same if it had not been done. The null hypothesis significance testing approach answers just that question: it tells you how likely your data are if the the null hypothesis was true. To do that, you compare the distribution of outcome scores in the intervention group and the control group. And as emphasized earlier, we don't just look at the difference in mean outcomes between two groups, we consider whether that difference is greater than you'd expect given the variation *within* the two groups. (This is what the term "analysis of variance" refers to).

## Steps to take before data analysis

-   General sanity check on dataset - are values within expected range?
-   Check assumptions
-   Plot data to get sense of likely range of results

### Sample dataset with illustrative analysis

To illustrate data analysis, we will use a real dataset that can be retrieved from the [ESRC data archive](https://reshare.ukdataservice.ac.uk/852291/) [@burgoyne2016]. We will focus only on a small subset of the data, which comes from an intervention study in which teaching assistants administered an individual reading and language intervention to children with Down syndrome. A wait-list RCT design was used (see Chapter \@ref(crossover)), but we will focus here on just the first two phases, in which half the children were randomly assigned to intervention, and the remainder formed a control group. Several language and reading measures were included in the study, giving a total of 11 outcomes. Here we will illustrate the analysis with just one of the outcomes - letter-sound coding - which was administered at baseline (t1) and immediately after the intervention (t2). Results from the full study have been reported by @burgoyne2012 and are discussed in the demonstration of MEff by @bishop2023b. <!---this is the measure with the strongest effect - I considered using others but they are not great for illustrative purposes, as they aren't very normal! Script below written so that we could easily substitute one of the other measures if we wished (but then text would also need redoing in places)-->

\begin{center}
```{r pirateRCT,echo=F,fig.cap='Data from RCT on language/reading intervention for Down syndrome by Burgoyne et al, 2012',warning=F,message=F,out.width="75%"}

makefig <- 0 # we preserve code to make figure for just historical reasons. NB this did not use ggplot and so can't use ggsave. Uses base R to make figure. Can open saved pdf in preview and crop and save as png

if(makefig==0){
  knitr::include_graphics("images_bw/pirateRCT.png")
}


dsdat <- read.csv("Data/dse-rli-trial-data-archive.csv")
dsdat <- dsdat[dsdat$included==1,]
dsdat$group <- as.factor(dsdat$group)

levels(dsdat$group)<-c("Intervention","Control")
dsdat$baseline <- dsdat$letter_sound_t1
dsdat$outcome <- dsdat$letter_sound_t2
dsdat$change<-dsdat$outcome-dsdat$baseline
if(makefig==1){
  pdf("images_bw/pirateRCT.pdf", width = 6, height = 5)
pirateplot(outcome~group,theme=1,cex.lab=1.3,data=dsdat, point.o=1, xlab="", ylab="Outcome score",pal="black")

#nb not part of ggplot
dev.off()
} 
```
\end{center}

Figure \@ref(fig:pirateRCT) shows results on letter-sound coding after one group had received the intervention. This test had also been administered at baseline, but we will focus first just on the outcome results.\index{visualization of data}\index{pirate plot}

Raw data \index{data!raw}should always be inspected prior to any data analysis, in order to just check that the distribution of scores looks sensible. One hears of horror stories where, for instance, an error code of 999 got included in an analysis, distorting all the statistics. Or where an outcome score was entered as 10 rather than 100. Visualizing the data is useful when checking whether the results are in the right numerical range for the particular outcome measure. There are many different ways of visualizing data: it is best to use one that shows the distribution of data, and even individual data points if the sample is not too big. @zhang2022 showed that the traditional way of presenting only means and error bars in figures can be misleading, and leads people to overestimate group differences.
A pirate plot such as that in \@ref(fig:pirateRCT) is one useful way of showing means and distributions as well as individual data points [@phillips2017].\index{data!visualization} 
A related step is to check whether the distribution of the data meets the assumptions of the proposed statistical analysis. Many common statistical procedures assume that data are normally distributed on an interval scale (see Chapter \@ref(reliability)). Statistical approaches to checking of assumptions are beyond the scope of this book, but there are good sources of information on the web, such as [this website](http://www.sthda.com/english/articles/39-regression-model-diagnostics/161-linear-regression-assumptions-and-diagnostics-in-r-essentials/) for linear regression. But just eyeballing the data is useful, and can detect obvious cases of non-normality, cases of ceiling or floor effects, or "clumpy" data, where only certain values are possible. Data with these features may need special treatment and it is worth consulting a statistician if they apply to your data. For the data in Figure \@ref(fig:pirateRCT), although neither distribution has an classically normal distribution, we do not see major problems with ceiling or floor effects, and there is a reasonable spread of scores in both groups.\index{non-normal data}\index{data!non-normal}
<!-- maybe add a figure showing egs of nonnormal distribution, ceiling effect, clumpy data-->

```{r table2gp,echo=F,out.width="75%"}
options(kableExtra.html.bsTable = T)
mytab <- psych::describeBy(dsdat$outcome, group=dsdat$group,mat=TRUE,digits=3)
mytab <- mytab[,c(2,4:6)]
colnames(mytab)<-c('Group','N','Mean','SD')
mytab[1:2,1]<-c("Intervention","Control")
row.names(mytab)<-NULL
knitr::kable(mytab,escape = F, align = "c", booktabs = T,caption='Summary statistics on letter-sound coding outcomes after intervention') %>%
  kable_styling(c("striped", "bordered"), latex_options = "striped", full_width = F)

```

The next step is just to compute some basic statistics to get a feel for the effect size. Table \@ref(tab:table2gp) shows the mean and standard deviation on the outcome measure for each group. The mean is the average of the individual datapoints shown in Figure \@ref(fig:pirateRCT), obtained by just summing all scores and dividing by the number of cases. The standard deviation gives an indication of the spread of scores around the mean, and as we have seen, is a key statistic for measuring an intervention effect. In these results, one mean is higher than the other, but there is overlap between the groups. Statistical analysis gives us a way of quantifying how much confidence we can place in the group difference: in particular, how likely is it that there is no real impact of intervention and the observed results just reflect the play of chance. In this case we can see that the difference between means is around 6 points and the average SD is around 8.

### Simple t-test on outcome data

The simplest way of measuring the intervention effect is to just compare outcome (outcome) measures on a t-test. We can use a one-tailed test with confidence, given that we anticipate outcomes will be better after intervention. One-tailed tests are often treated with suspicion, because they can be used by researchers engaged in p-hacking (see Chapter \@ref(phacking)), but where we predict a directional effect, they are entirely appropriate and give greater power than a two-tailed test: see [this blogpost by Daniël Lakens](http://daniellakens.blogspot.com/2016/03/one-sided-tests-efficient-and-underused.html).\index{one-tailed test}  
When reporting the result of a t-test, researchers should always report all the statistics: the value of t, the degrees of freedom, the means and SDs, and the confidence interval around the mean difference, as well as the p-value. This not only helps readers understand the magnitude and reliability of the effect of interest: it also allows for the study to readily be incorporated in a meta-analysis. Results from a t-test for the data in Table \@ref(tab:table2gp) are shown in Table \@ref(tab:ttestoutcomes). Note that with a one-tailed test, the confidence interval on one side will extend to infinity: this is because a one-tailed test assumes that the true result is greater than a specified mean value, and disregards results that go in the opposite direction. \index{t-test}

```{r ttestoutcomes,echo=F,warning=F,message=F}
options(kableExtra.html.bsTable = T)
myt1 <- t.test(dsdat$outcome~dsdat$group,var.equal=T,alternative='greater')

mytabt <- c(round(myt1$statistic,3),round(myt1$parameter,0), round(myt1$p.value,3),round(myt1$estimate[1]-myt1$estimate[2],3),round(myt1$conf.int,3))
mytabt <- as.matrix(mytabt,ncol=6)
mytabt <- as.data.frame(t(mytabt))
colnames(mytabt)<-c('t','df','p','mean diff.','lowerCI','upperCI')
row.names(mytabt)<-NULL
knitr::kable(mytabt,escape = F, align = "c", booktabs = T,caption='T-test on outcomes') %>%
  kable_styling(c("striped", "bordered"), latex_options = "striped", full_width = F)
```

### T-test on difference scores

The t-test on outcomes is easy to do, but it misses an opportunity to control for one unwanted source of variation, namely individual differences in the initial level of the language measure. For this reason, researchers often prefer to take difference scores: the difference between outcome and baseline measures, and apply a t-test to these. In fact, that is what we did in the fictional examples of dietary intervention above: we reported *weight loss* rather than weight for each group before and after intervention. That seems sensible, as it means we can ignore individual differences in initial weights in the analysis.\
While this had some advantages over reliance on raw outcome measures, it also has disadvantages, because the amount of change that is possible from baseline to outcome is not the same for everyone. A child with a very low score at baseline has more "room for improvement" than one who has an average score. For this reason, analysis of difference scores is not generally recommended.

### Analysis of covariance on outcome scores

Rather than taking difference scores, it is preferable to analyse differences in outcome measures after making a statistical adjustment that takes into account the initial baseline scores, using a method known as analysis of covariance or ANCOVA. In practice, this method usually gives results that are similar to those you would obtain from an analysis of difference scores, but the precision is greater, making it easier to detect a true effect. However, the data do need to meet certain assumptions of the method. This [website](https://www.datanovia.com/en/lessons/ancova-in-r/) walks through the steps for performing an ANCOVA in R, starting with a plot to check that there is a linear relationship between pretest vs posttest scores in both groups - i.e. the points cluster around a straight line, as shown in Figure \@ref(fig:ds-prepost).\index{analysis!analysis of variance}\index{analysis!analysis of covariance}


```{r ds-prepost,echo=F,fig.cap='Baseline vs outcome scores in the Down syndrome data',warning=F,message=F,out.width="80%"}
makefig <- 0 # we preserve code to make figure for just historical reasons. NB this did not use ggplot and so can't use ggsave. Uses base R to make figure.

if(makefig==0){
  knitr::include_graphics("images_bw/ds-prepost.png")
}

if(makefig==1){
#plot to check linearity by eye
ggscatter(
  dsdat, x = "baseline", y = "outcome",
  color = "group", add = "reg.line"
  )+
    xlab("Baseline")+ylab("Outcome")+
  scale_colour_grey() + theme_bw()
  ggsave('images_bw/ds-prepost.png',scale = 1, width = 6, height = 4, units = c("in"), dpi = 300)
}
```


Inspection of the plot confirms that the relationship between pretest and outcome looks reasonably linear in both groups. Note that it also shows that there are rather more children with very low scores at pretest in the control group. This is just a chance finding - the kind of thing that can easily happen when you have relatively small numbers of children randomly assigned to groups.

```{r ancova-compare,echo=F,warning=F,message=F}
options(kableExtra.html.bsTable = T)
#https://www.datanovia.com/en/lessons/ancova-in-r/

## I've omitted checks from this website for now, as I think this is too much detail. We want focus on how ancova gives different results from simple t-test because it adjusts for baseline scores. 

# order of variables matters when computing ANCOVA. You want to remove the effect of the covariate first - that is, you want to control for it - prior to entering your main variable or interest.
aov.1 <- dsdat %>% anova_test(outcome ~ group)
res.aov <- dsdat %>% anova_test(outcome ~ baseline +group)
# mylm1 <- lm(posttest~group,data=dsdat) #equivalent to aov.1
mylm <- lm(outcome~baseline+group,data=dsdat) #equivalent to res.aov
tab1 <- as.data.frame(get_anova_table(aov.1))
tab2 <- as.data.frame(get_anova_table(res.aov))
tab3 <- as.data.frame(summary(mylm)$coefficients)
source <-c('Intercept','baseline','Group')
tab3 <- cbind(source,tab3)
colnames(tab3)[5]<-'p'
tab1$p <- round(tab1$p,3)
tab2$p <- round(tab2$p,3)
tab3$p <- round(tab3$p,3)


tab1<-tab1[,-6]
tab2<-tab2[,-6]


knitr::kable(tab1,escape = F, align = "c", booktabs = T,caption='Analysis of outcome only (ANOVA)') %>%
  kable_styling(c("striped", "bordered"), latex_options = "striped", full_width = F) %>%
  column_spec(1, bold = T) 
```



Table \@ref(tab:ancova-compare) shows the Down syndrome data analysed using ANOVA to compare only the outcome scores (upper chart), and Table \@ref(tab:ancova) shows the same data analysed using ANCOVA to adjust scores for the baseline values. 
```{r ancova,echo=F,warning=F,message=F}
knitr::kable(tab2,escape = F, align = "c", booktabs = T,caption='Outcome adjusted for pretest scores (ANCOVA)') %>%
  kable_styling(c("striped", "bordered"), latex_options = "striped", full_width = F)

```
The effect size is shown as *ges*, which stands for "generalized eta squared". You can see there is a large *ges* value, and correspondingly low p-value for the baseline term, reflecting the strong correlation between baseline and outcome shown in Figure \@ref(fig:ds-prepost). In effect, with ANCOVA, we adjust scores to remove the effect of the baseline on the outcome scores; in this case, we can then see a slightly stronger effect of the intervention: the effect size for the group term is higher and the p-value is lower than with the previous ANOVA.  

For readers who are not accustomed to interpreting statistical output, the main take-away message here is that you get a better estimate of the intervention effect if the analysis uses a statistical adjustment that takes into account the baseline scores.

<!---#### T-tests, analysis of variance, and linear regression {.unnumbered}

<p id="custom">-->
\begin{tcolorbox}[colback=Black!5!lightgray,colframe=black!75!black,coltitle=white,title=T-tests, analysis of variance, and linear regression]\label{box:ttestcomparison}
Mathematically, the t-test is equivalent to two other methods: analysis of variance and linear regression. When we have just two groups, all of these methods achieve the same thing: they divide up the variance in the data into variance associated with group identity, and other (residual) variance, and provide a statistic that reflects the ratio between these two sources of variance. This is illustrated with the real data analysed in Table \@ref(tab:ancova-compare). The analysis gives results that are equivalent to the t-test in Table \@ref(tab:ttestoutcomes): if you square the t-value it is the same as the F-test. In this case, the p-value from the t-test is half the value of that in the ANOVA: this is because we specified a one-tailed test for the t-test. The p-value would be identical to that from ANOVA if a two-tailed t-test had been used.\
See [this blogpost](http://deevybee.blogspot.com/2017/11/anova-t-tests-and-regression-different.html) for more details. <!-- reminder to self: blogposts referenced in the book should be uploaded to figshare or another place with DOI-->
<!---</p>-->
\end{tcolorbox}  

### Non-normal data

We've emphasised that the statistical methods reviewed so far assume that data will be reasonably normally distributed. This raises the question of what to do if we have results that don't meet that assumption. There are two answers that are typically considered. The first is to use **non-parametric statistics**,\index{non-parametric statistics} which do not require normally-distributed data. For instance, the **Mann-Whitney U test**\index{Mann-Whitney test} can be used in place of a t-test. An alternative approach is to **transform** the data to make it more normal.\index{transformation of data}\index{data!transformation} For instance, if the data is skewed with a long tail of extreme values, a log transform may make the distribution more normal. This is often appropriate if, for instance, measuring response times. Many people are hesitant about transforming data - they may feel it is wrong to manipulate scores in this way. There are two answers to that. First, it does not make sense to be comfortable with nonparametric statistics and uncomfortable with data transformation, because nonparametric methods involve transformation - typically they work by rank ordering data, and then working with the ranks, rather than the original scores. Second, there is nothing inherently correct about measuring things on a linear scale: scientists often use log scales to measure quantities that span a wide range, such as the Richter scale for earthquakes or the decibel scale for sound volume. So, provided the transformation is justified as helping make the data more suited for statistical analysis, it is a reasonable thing to do. Of course, it is *not* appropriate to transform data just to try and nudge data into significance. Nor is it a good idea to just wade in an apply a data transformation without first looking at the data to try and understand causes of non-normality. For instance, if your results showed that 90% of people had no effect of intervention, and 10% had a huge effect, it might make more sense to consider a different approach to analysis altogether, rather than shoe-horning results into a more normal distribution. We will not go into details of nonparametric tests and transformations here, as there are plenty of good sources of information available. We recommend consulting a statistician if data seem unsuitable for standard parametric analysis. 

### Linear mixed models (LMM) approach

\index{Linear mixed models}Increasingly, reports of RCTs are using more sophisticated and flexible methods of analysis that can, for instance, cope with datasets that have missing data, or where distributions of scores are non-normal. 

```{r indsubs-plot,echo=F,include=F,fig.cap = "Individual subjects' pretest and posttest scores",warning=F,message=F}
#trying to follow steps from:
#https://www.crumplab.com/psyc7709_2019/book/docs/a-tutorial-for-using-the-lme-function-from-the-nlme-package-.html
#put data in long format
vars1<-c('subject_id','group','baseline')
vars2<-c('subject_id','group','outcome')
half1<-dsdat[,vars1]
half2<-dsdat[,vars2]
colnames(half1)[3]<-'measurement'
colnames(half2)[3]<-'measurement'
long_all <- rbind(half1,half2)
long_all$time<-'Outcome'
long_all$time[1:nrow(dsdat)]<-'Baseline'
long_all$dummy<-1
long_all$dummy[1:nrow(dsdat)]<-0
colnames(long_all)[1]<-'ID'
long_all$ID<-as.factor(long_all$ID)

long_all$jittered <- jitter(long_all$measurement)
plot<- ggplot(long_all, aes(x=time, y=jittered,  color=group, group = ID), xlab(time) ) + 
  geom_point(size=.75)+
  geom_line(color="grey")+
    ylab("Score")+xlab("Session")
plot
#plot needs jitter because otherwise confusing because scores are in whole numbers so you get overlaid points - but even with jitter, not all that informative!.
```

```{r lmer-ds,echo=F,include=F,tab.cap='Output from Linear Mixed Model'}

mymodel <- lmer(measurement~time*group + (1|ID),  data = long_all)
sumbit <- anova(mymodel)

#null model
 nullmodel <- lmer(measurement~1 + (1|ID),  data = long_all)
# summary(nullmodel)

sumbit



```

An advantage of the LMM approach is that it can be extended in many ways to give appropriate estimates of intervention effects in more complex trial designs - some of which are covered in Chapter \@ref(adaptive) to Chapter \@ref(Single). Disadvantages of this approach are that it is easy to make mistakes in specifying the model for analysis if you lack statistical expertise, and the output is harder for non-specialists to understand. If you have a simple design, such as that illustrated in this chapter, with normally distributed data, a basic analysis of covariance is perfectly adequate [@oconnell2017].

<!--- Paul's help needed here?  I had assumed LMM would give more precise estimates, but the OConnell paper suggests ANCOVA is optimal-->

Table \@ref(tab:table-procon) summarises the pros and cons of different analytic approaches.

```{r table-procon, echo=F, include=T,out.width="75%"}
options(kableExtra.html.bsTable = T)
mytests <- c('t-test','ANOVA','Linear regression/ ANCOVA', 'LMM')
mytext1 <- c('Good power with 1-tailed test. \nSuboptimal control for baseline. \nAssumes normality.',
             'With two-groups, equivalent to t-test, \nbut two-tailed only. \nCan extend to more than two groups.',
             'Similar to ANOVA, but can adjust \noutcomes for covariates, \nincluding baseline.',
             'Flexible in cases with missing data, \nnon-normal distributions.')
mytext2 <- c('High','...','...','Low')
mytext3 <- c('Low','...','...','High')

mydf<-data.frame(cbind(mytests,mytext1,mytext2,mytext3))
colnames(mydf) <- c('Method','Features','Ease of understanding','Flexibility')
knitr::kable(mydf,escape = T, align = "c", booktabs = T,caption="Analytic methods for comparing outcomes in intervention vs control groups") %>%
  kable_styling(c("striped", "bordered"), latex_options = "striped", full_width = F) %>%
  column_spec(1:1, bold = T) %>%
  column_spec(1:4,width = "7em") %>%
  column_spec(2,width = "12em")




```

## Check your understanding

1.  Figure \@ref(fig:boxploteg) shows data from a small intervention trial in the form of a boxplot. If you are not sure what this represents, try Googling for more information. Check your understanding of the annotation in the top left of the plot. How can you interpret the p-value? What does the 95% CI refer to? Is the "mean.diff" value the same as shown in the plot?\index{boxplot}

\begin{center}
```{r boxploteg,echo=F,message=F,fig.cap ='Sample data for t-testin boxplot format',out.width="60%"}
# Data in two numeric vectors
# Modified from stdha example on ttest (on weight)
makefig<-0
if(makefig==0){
  knitr::include_graphics("images_bw/boxeg.png")
}
if(makefig==1){
women_weight <- c(48.9, 61.2, 73.3, 81.8, 63.4, 64.6, 48.4, 48.8, 48.5)
men_weight <- c(67.8, 60, 63.4, 76, 89.4, 73.3, 67.3, 61.3, 62.4) 
# Create a data frame
my_data <- data.frame( 
                group = rep(c("Control", "Intervention"), each = 9),
                score = c(women_weight,  men_weight)
                )
myt <- t.test(women_weight, men_weight, var.equal = TRUE)
mylabel=paste0("t = ",round(-myt$statistic,2),"; DF = ",myt$parameter,"; p = ",round(myt$p.value,3),"; \nmean diff. = ",round((myt$estimate[2]-myt$estimate[1]),2),"; 95% CI  = ",round(-myt$conf.int[2],2)," to ", round(-myt$conf.int[1],2))
myg <- ggboxplot(my_data, x = "group", y = "score",
        ylab = "Score", xlab = "Groups")+
  annotate(geom="text",x=1.2, y=88, label=mylabel)


ggsave('images_bw/boxeg.png',myg,scale = 1, width = 6, height = 5, units = c("in"), dpi = 300)
}
```
\end{center}




2. Find an intervention study of interest and check which analysis method was used to estimate the intervention effect. Did the researchers provide enough information to give an idea of the effect size, or merely report p-values? Did the analysis method take into account baseline scores in an appropriate way?


